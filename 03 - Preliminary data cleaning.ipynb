{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92e2ecb1",
   "metadata": {},
   "source": [
    "# 03 - Preliminary data cleaning\n",
    "____\n",
    "\n",
    "Step 3! We have our pristine dataset split nicely into our training (and validation) and testing data. \n",
    "\n",
    "It is important to remember that any steps / transformations we apply to our training data must also be applied to the test data to avoid any errors. If we're going to drop columns, lets build up a list of cols_to_drop so we can apply this nice and easily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6a3a744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78bf20e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('training_data.csv', low_memory = False, na_values=[-1, '-1', 'Data missing or out of range', 'Unknown'])\n",
    "test = pd.read_csv('testing_data.csv', low_memory = False, na_values=[-1, '-1', 'Data missing or out of range', 'Unknown'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cd8857",
   "metadata": {},
   "source": [
    "## Let's have a look at our data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4818108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11359 entries, 0 to 11358\n",
      "Data columns (total 57 columns):\n",
      " #   Column                                       Non-Null Count  Dtype  \n",
      "---  ------                                       --------------  -----  \n",
      " 0   accident_index                               11359 non-null  int64  \n",
      " 1   accident_year_x                              11359 non-null  int64  \n",
      " 2   accident_reference_x                         11359 non-null  int64  \n",
      " 3   location_easting_osgr                        11359 non-null  float64\n",
      " 4   location_northing_osgr                       11359 non-null  float64\n",
      " 5   longitude                                    11359 non-null  float64\n",
      " 6   latitude                                     11359 non-null  float64\n",
      " 7   police_force                                 11359 non-null  object \n",
      " 8   accident_severity                            11359 non-null  object \n",
      " 9   number_of_vehicles                           11359 non-null  int64  \n",
      " 10  number_of_casualties                         11359 non-null  int64  \n",
      " 11  date                                         11359 non-null  object \n",
      " 12  day_of_week                                  11359 non-null  object \n",
      " 13  time                                         11359 non-null  object \n",
      " 14  local_authority_district                     4769 non-null   object \n",
      " 15  local_authority_ons_district                 11359 non-null  object \n",
      " 16  local_authority_highway                      11359 non-null  object \n",
      " 17  first_road_class                             11359 non-null  object \n",
      " 18  first_road_number                            4982 non-null   object \n",
      " 19  road_type                                    11286 non-null  object \n",
      " 20  speed_limit                                  11359 non-null  int64  \n",
      " 21  junction_detail                              11359 non-null  object \n",
      " 22  junction_control                             5948 non-null   object \n",
      " 23  second_road_class                            11359 non-null  object \n",
      " 24  second_road_number                           4468 non-null   object \n",
      " 25  pedestrian_crossing_human_control            11336 non-null  object \n",
      " 26  pedestrian_crossing_physical_facilities      11337 non-null  object \n",
      " 27  light_conditions                             11359 non-null  object \n",
      " 28  weather_conditions                           11161 non-null  object \n",
      " 29  road_surface_conditions                      11330 non-null  object \n",
      " 30  special_conditions_at_site                   255 non-null    object \n",
      " 31  carriageway_hazards                          260 non-null    object \n",
      " 32  urban_or_rural_area                          11359 non-null  object \n",
      " 33  did_police_officer_attend_scene_of_accident  11359 non-null  object \n",
      " 34  trunk_road_flag                              11359 non-null  object \n",
      " 35  lsoa_of_accident_location                    11359 non-null  object \n",
      " 36  enhanced_severity_collision                  11357 non-null  float64\n",
      " 37  accident_year_y                              11359 non-null  int64  \n",
      " 38  accident_reference_y                         11359 non-null  int64  \n",
      " 39  vehicle_reference                            11359 non-null  int64  \n",
      " 40  casualty_reference                           11359 non-null  int64  \n",
      " 41  casualty_class                               11359 non-null  object \n",
      " 42  sex_of_casualty                              11354 non-null  object \n",
      " 43  age_of_casualty                              11088 non-null  float64\n",
      " 44  age_band_of_casualty                         11088 non-null  object \n",
      " 45  casualty_severity                            11359 non-null  object \n",
      " 46  pedestrian_location                          11359 non-null  object \n",
      " 47  pedestrian_movement                          11359 non-null  object \n",
      " 48  car_passenger                                11345 non-null  object \n",
      " 49  bus_or_coach_passenger                       11346 non-null  object \n",
      " 50  pedestrian_road_maintenance_worker           11350 non-null  object \n",
      " 51  casualty_type                                11359 non-null  object \n",
      " 52  casualty_home_area_type                      8642 non-null   object \n",
      " 53  casualty_imd_decile                          8634 non-null   object \n",
      " 54  lsoa_of_casualty                             8596 non-null   object \n",
      " 55  enhanced_casualty_severity                   11357 non-null  float64\n",
      " 56  casualty_distance_banding                    8634 non-null   float64\n",
      "dtypes: float64(8), int64(10), object(39)\n",
      "memory usage: 4.9+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fac0f60",
   "metadata": {},
   "source": [
    "Firstly, we have a lot of features. Feature selection will be very important to make sure that our model doesn't overfit. There are also a lot of superfluous features that can easily be dropped at our feature selection stage:\n",
    "  * The geographical location columns \n",
    "  * The duplicate accident_year and accident_reference columns\n",
    "  * Whether or not a police officer attended *after* the collision occured won't have affected the seriousness\n",
    "  * vehicle and casualty reference isn't useful\n",
    "  * accident_severity should be dropped as it is highly correlated to the casualty_severity, and will just be a proxy for the model to overfit to\n",
    "  * same with the enhanced severity\n",
    "\n",
    "It also looks like we have a tonne of missing data, so first let's make sure that we drop features that have so much missing they won't really assist in our predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be072972",
   "metadata": {},
   "source": [
    "### Let's drop columns where we're missing a lot of data.\n",
    "\n",
    "We have a lot of NaN values, but also some more disguised missing data. Some entries are labelled with -1 (both text and numeric present), 'Data missing or out of range', and 'Unknown'. Missing greater than a third of data is going to limit our predictive power so let's remove these columns from the off. We've specified the alternative values for the NaN values in our read_csv() function above, so these will be counted with typical NaN based methods.\n",
    "\n",
    "We will evaluate the missing data in the training data set, and apply any column dropping to both datasets. No peeking at the testing data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba8aba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = []\n",
    "\n",
    "columns_to_drop.extend(train.columns[(train.isna().sum() / train.shape[0]) * 100 > 5].to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f65c989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['local_authority_district', 'first_road_number', 'junction_control', 'second_road_number', 'special_conditions_at_site', 'carriageway_hazards', 'casualty_home_area_type', 'casualty_imd_decile', 'lsoa_of_casualty', 'casualty_distance_banding']\n"
     ]
    }
   ],
   "source": [
    "print(columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f61c22",
   "metadata": {},
   "source": [
    "37% of local authority district data missing, but we have the full dataset for ONS district. Let's use this to infill the local authority district and then drop the ONS district column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "397296ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in [train, test]:\n",
    "  districts = {'E08000019': 'Sheffield', 'E08000017': 'Doncaster', 'E08000018': 'Rotherham', 'E08000016': 'Barnsley'}\n",
    "\n",
    "  for ind,row in enumerate(data.local_authority_district):\n",
    "    if row is np.nan:\n",
    "      district = districts[data.local_authority_ons_district.iloc[ind]]\n",
    "      data.local_authority_district.iloc[ind] = district\n",
    "\n",
    "columns_to_drop.append('local_authority_ons_district')\n",
    "columns_to_drop.remove('local_authority_district')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03602749",
   "metadata": {},
   "source": [
    "#### Now we're ready to drop the columns with >5% of the data missing. We'll apply this to both the test and train data, and output the saved data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38443432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[data.drop(columns=columns_to_drop, inplace=True) for data in [train, test]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12166f5",
   "metadata": {},
   "source": [
    "#### Which columns still have missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b4d46f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age_of_casualty                            271\n",
      "age_band_of_casualty                       271\n",
      "weather_conditions                         198\n",
      "road_type                                   73\n",
      "road_surface_conditions                     29\n",
      "pedestrian_crossing_human_control           23\n",
      "pedestrian_crossing_physical_facilities     22\n",
      "car_passenger                               14\n",
      "bus_or_coach_passenger                      13\n",
      "pedestrian_road_maintenance_worker           9\n",
      "sex_of_casualty                              5\n",
      "enhanced_severity_collision                  2\n",
      "enhanced_casualty_severity                   2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cols_missing_data = train.loc[:, train.isnull().any()].isna().sum().sort_values(ascending=False)\n",
    "print(cols_missing_data)\n",
    "missing_numeric = train[cols_missing_data.index].describe().columns\n",
    "missing_categorical = train.describe(exclude=[np.number]).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8785e5",
   "metadata": {},
   "source": [
    "The missing data for these features represents quite a small proportion of the dataset, so let's just infill with the most frequent value for categories, or the mean for numerics. We'll apply this to all columns in the dataframe incase our testing data has missing data in other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e29fc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[data[col].fillna(data[col].mean(), inplace=True) for col in (data.describe().columns)] for data in [train, test]]\n",
    "\n",
    "for data in [train, test]:\n",
    "  categorical_cols = data.describe(exclude=[np.number]).columns\n",
    "  data[categorical_cols] = data[categorical_cols].apply(lambda x: x.fillna(x.value_counts().index[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4c9fab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11359 entries, 0 to 11358\n",
      "Data columns (total 47 columns):\n",
      " #   Column                                       Non-Null Count  Dtype  \n",
      "---  ------                                       --------------  -----  \n",
      " 0   accident_index                               11359 non-null  int64  \n",
      " 1   accident_year_x                              11359 non-null  int64  \n",
      " 2   accident_reference_x                         11359 non-null  int64  \n",
      " 3   location_easting_osgr                        11359 non-null  float64\n",
      " 4   location_northing_osgr                       11359 non-null  float64\n",
      " 5   longitude                                    11359 non-null  float64\n",
      " 6   latitude                                     11359 non-null  float64\n",
      " 7   police_force                                 11359 non-null  object \n",
      " 8   accident_severity                            11359 non-null  object \n",
      " 9   number_of_vehicles                           11359 non-null  int64  \n",
      " 10  number_of_casualties                         11359 non-null  int64  \n",
      " 11  date                                         11359 non-null  object \n",
      " 12  day_of_week                                  11359 non-null  object \n",
      " 13  time                                         11359 non-null  object \n",
      " 14  local_authority_district                     11359 non-null  object \n",
      " 15  local_authority_highway                      11359 non-null  object \n",
      " 16  first_road_class                             11359 non-null  object \n",
      " 17  road_type                                    11359 non-null  object \n",
      " 18  speed_limit                                  11359 non-null  int64  \n",
      " 19  junction_detail                              11359 non-null  object \n",
      " 20  second_road_class                            11359 non-null  object \n",
      " 21  pedestrian_crossing_human_control            11359 non-null  object \n",
      " 22  pedestrian_crossing_physical_facilities      11359 non-null  object \n",
      " 23  light_conditions                             11359 non-null  object \n",
      " 24  weather_conditions                           11359 non-null  object \n",
      " 25  road_surface_conditions                      11359 non-null  object \n",
      " 26  urban_or_rural_area                          11359 non-null  object \n",
      " 27  did_police_officer_attend_scene_of_accident  11359 non-null  object \n",
      " 28  trunk_road_flag                              11359 non-null  object \n",
      " 29  lsoa_of_accident_location                    11359 non-null  object \n",
      " 30  enhanced_severity_collision                  11359 non-null  float64\n",
      " 31  accident_year_y                              11359 non-null  int64  \n",
      " 32  accident_reference_y                         11359 non-null  int64  \n",
      " 33  vehicle_reference                            11359 non-null  int64  \n",
      " 34  casualty_reference                           11359 non-null  int64  \n",
      " 35  casualty_class                               11359 non-null  object \n",
      " 36  sex_of_casualty                              11359 non-null  object \n",
      " 37  age_of_casualty                              11359 non-null  float64\n",
      " 38  age_band_of_casualty                         11359 non-null  object \n",
      " 39  casualty_severity                            11359 non-null  object \n",
      " 40  pedestrian_location                          11359 non-null  object \n",
      " 41  pedestrian_movement                          11359 non-null  object \n",
      " 42  car_passenger                                11359 non-null  object \n",
      " 43  bus_or_coach_passenger                       11359 non-null  object \n",
      " 44  pedestrian_road_maintenance_worker           11359 non-null  object \n",
      " 45  casualty_type                                11359 non-null  object \n",
      " 46  enhanced_casualty_severity                   11359 non-null  float64\n",
      "dtypes: float64(7), int64(10), object(30)\n",
      "memory usage: 4.1+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50792931",
   "metadata": {},
   "source": [
    "Let's get rid of the duplicated columns from merging the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96314e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in [train, test]:\n",
    "  data.rename(columns={'accident_year_x': 'accident_year', 'accident_reference_x': 'accident_reference'}, inplace=True)\n",
    "  data.drop(labels=['accident_year_y', 'accident_reference_y', 'police_force'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd437e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('cleaned_training_data.csv', index=False)\n",
    "test.to_csv('cleaned_testing_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b373219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DfT_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
