{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Establishing  baseline performance\n",
    "____\n",
    "\n",
    "Before we go any further, we want to make sure that any feature selection or engineering that we do is actually useful!\n",
    "\n",
    "The best way to do this is to establish a baseline performance for a model that is just fed the data as is. We can also compare our results to a 'dummy classifier' that will just guess the classifications - because we have an unbalanced predictor, we will just predict the most common class ('slight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.dummy import DummyClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick data preparation\n",
    "* Drop unique indexing columns\n",
    "* OHE columns and seperate X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('cleaned_training_data.csv', low_memory = False)\n",
    "\n",
    "severity_dict = {'Slight': 0, 'Serious': 1}\n",
    "\n",
    "y_train = train.casualty_severity\n",
    "y_train.replace(severity_dict, inplace=True)\n",
    "\n",
    "\n",
    "X_train = train.drop(['accident_severity', 'accident_reference', 'vehicle_reference', 'casualty_reference' ], axis=1)\n",
    "X_train = pd.get_dummies(X_train, prefix_sep='_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('cleaned_training_data.csv', low_memory = False)\n",
    "\n",
    "y_test = test.casualty_severity\n",
    "y_test.replace(severity_dict, inplace=True)\n",
    "\n",
    "\n",
    "X_test = test.drop(['accident_severity', 'accident_reference', 'vehicle_reference', 'casualty_reference' ], axis=1)\n",
    "X_test = pd.get_dummies(X_test, prefix_sep='_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Classifier\n",
    "Let's create a \"dummy classifier\" that makes predictions without any input features. This will be our simple baseline to compare our more complex classifiers.\n",
    "\n",
    "Here, we are predicting everything to have the 'majority' label - 0. Because the classes are imbalanced (the majority of data has a true label of 0) this dummy classfier achieves a high accuracy of __72%__, i.e. is the same % as the majority label in the dataset. However, this is clearly not actually a useful predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Most Frequent Class Dummy Classifier): 0.72\n"
     ]
    }
   ],
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_freq_pred = dummy_clf.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "print(\"Accuracy (Most Frequent Class Dummy Classifier):\", np.round(accuracy_score(y_test, dummy_freq_pred),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the \"stratified\" strategy to generate random predictions based on the class distribution in the dataset. This means the classifier is not biased towards the majority. Now we have an accuracy of __60%__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Stratified Class Dummy Classifier): 0.59\n"
     ]
    }
   ],
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"stratified\")\n",
    "dummy_strat_pred = dummy_clf.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "print(\"Accuracy (Stratified Class Dummy Classifier):\", np.round(accuracy_score(y_test, dummy_strat_pred),2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This highlights the importance of not evaluating our models based on accuracy alone! 'recall' gives us the ratio of correctly predicted Serious casualties out of all of the Serious casualties in the dataset. 'precision' will tell us how many of our Serious predictions were actually correct.\n",
    "\n",
    "As we would like to be able to accurately pick out which factors are contributing to serious collisions, we would like to balance both our precision (how many of our serious predictions were correct) and our recall (how well did we classify the serious collisions in the dataset). Let's therefore focus on the F1 score.\n",
    "\n",
    "The F1 score is the harmonic mean of the preicion and recall, taking into account both false positives and false negatives. THis is particularly imporant considering the imbalanced classes in our dataset where accuracy will not really give us any information about our predictive power.\n",
    "\n",
    "We should also investigate the area-under-the-curve of the precision-recall curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report (Most Frequent Class Dummy Classifier):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      1.00      0.84      8173\n",
      "           1       0.00      0.00      0.00      3186\n",
      "\n",
      "    accuracy                           0.72     11359\n",
      "   macro avg       0.36      0.50      0.42     11359\n",
      "weighted avg       0.52      0.72      0.60     11359\n",
      "\n",
      "\n",
      "Classification Report (Stratified Class Dummy Classifier):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.72      0.72      8173\n",
      "           1       0.27      0.27      0.27      3186\n",
      "\n",
      "    accuracy                           0.59     11359\n",
      "   macro avg       0.50      0.50      0.50     11359\n",
      "weighted avg       0.59      0.59      0.59     11359\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Elena\\.conda\\envs\\DfT_project_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Elena\\.conda\\envs\\DfT_project_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Elena\\.conda\\envs\\DfT_project_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClassification Report (Most Frequent Class Dummy Classifier):\")\n",
    "print(classification_report(y_test, dummy_freq_pred))\n",
    "\n",
    "print(\"\\nClassification Report (Stratified Class Dummy Classifier):\")\n",
    "print(classification_report(y_test, dummy_strat_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our classification report, we can see that a value of 0.28 in the F1 score is the value to try and beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DfT_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
